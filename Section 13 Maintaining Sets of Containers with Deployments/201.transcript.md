# **Tutor Notes — Updating a Deployment When a New Image Version Is Available**

## Objective

Understand why **Kubernetes deployments don’t automatically pull new image versions** and explore **three possible solutions** to force updates when new images are pushed to Docker Hub.

---

## Problem Overview

When an updated image (e.g., `multi-client`) is pushed to Docker Hub, Kubernetes deployments **don’t automatically detect or pull** that new image.

Re-applying the same configuration file (via `kubectl apply -f`) doesn’t trigger a pod update because:

* The configuration file hasn’t changed.
* `kubectl apply` detects **no difference** and skips any action.
* Kubernetes never re-pulls the image if the spec is unchanged.

Example:

```bash
kubectl apply -f client-deployment.yaml
```

Output:

```
deployment.apps/client-deployment unchanged
```

So, even if the image on Docker Hub is newer, the deployment keeps running pods with the old image.

---

## Why This Happens

* The deployment YAML references the image **without a version tag** or uses a static tag like `latest`.
* Kubernetes considers the spec identical to the last applied state.
* It does **not check Docker Hub for updates** of the same tag.
* Therefore, **no new pods are created** and **no image is pulled**.

---

## Reference

This issue is discussed in Kubernetes GitHub **Issue #33664**, where contributors describe the challenges and workarounds for forcing deployments to use updated images.

---

## Possible Solutions

### **1. Manually Delete Pods**

Delete all pods managed by the deployment:

```bash
kubectl get pods
kubectl delete pod <pod-name>
```

**Behavior:**

* The deployment controller detects missing pods.
* It automatically recreates them using the pod template.
* When recreated, the pod **may pull the latest image** from Docker Hub (depending on imagePullPolicy).

**Downsides:**

* Risk of deleting the wrong pods.
* Brief downtime — no running pods to serve requests.
* Not practical in production environments.

**Verdict:** ❌ Not recommended.

---

### **2. Use Versioned Image Tags**

Tag images with explicit versions during the build process:

```bash
docker build -t <docker-id>/multi-client:v1 .
docker push <docker-id>/multi-client:v1
```

Then, update the deployment YAML:

```yaml
containers:
  - name: client
    image: <docker-id>/multi-client:v1
```

Apply the change:

```bash
kubectl apply -f client-deployment.yaml
```

**Why this works:**

* Changing the tag (e.g., `v1 → v2`) counts as a real config change.
* Kubernetes redeploys new pods with the updated image.

**Downsides:**

* Requires manually managing version tags and updating the YAML each time.
* CI/CD systems must handle synchronization between:

  * Image version tagging, and
  * YAML version updates + commits.
* Environment variables **cannot** be used in YAML to automate this.

**Verdict:** ⚠️ Works reliably but adds manual steps and complexity.

---

### **3. Use an Imperative Command**

After pushing a new image version, directly tell Kubernetes to update the deployment image:

```bash
kubectl set image deployment/client-deployment client=<docker-id>/multi-client:v2
```

**What happens:**

* This triggers an immediate rolling update.
* The deployment replaces old pods with new ones using the specified image.
* No YAML file edits are required.

**Downsides:**

* Imperative (not declarative).
* The YAML file no longer reflects the actual running configuration.
* Can cause drift between the source-of-truth manifest and live cluster state.

**Verdict:** ✅ Most practical solution in CI/CD pipelines.
Used in automated deployment scripts to update running pods after image builds.

---

## Comparison Summary

| **Method**              | **Approach**                 | **Pros**                      | **Cons**                           |
| ----------------------- | ---------------------------- | ----------------------------- | ---------------------------------- |
| 1. Delete Pods          | Manually remove running pods | Simple, triggers new pods     | Risky, downtime, manual            |
| 2. Version Tags in YAML | Update YAML with `v1`, `v2`  | Declarative, consistent       | Manual version sync, CI complexity |
| 3. Imperative Command   | `kubectl set image`          | Fast, scriptable, CI-friendly | Not declarative, YAML drift        |

---

## Recommended Practice

In production CI/CD pipelines:

1. **Build and tag** images uniquely (e.g., `v1`, `v2`, or with Git SHA).
2. **Push** to Docker Hub.
3. **Run an imperative update command** to roll out the new version:

   ```bash
   kubectl set image deployment/client-deployment client=<docker-id>/multi-client:v2
   ```

This approach ensures controlled updates with minimal downtime while avoiding complex YAML version management.

---

### Key Takeaways

* Kubernetes doesn’t automatically detect updated images.
* Reapplying an unchanged deployment file does nothing.
* The **imperative image update command** is the most reliable practical method.
* For production, use **unique image tags + automated rollout commands** for predictable updates.
